{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGES MADE TO CORE CODE 4/07/2019:\n",
    "comment flatten method\n",
    "\n",
    "extracted roll axis from flatten - added\n",
    "\n",
    "added roll axis method - added\n",
    "\n",
    "take concat out of processor, processor returns 3D array - added\n",
    "\n",
    "put concat into umap model - added\n",
    "\n",
    "edit vae in utils to expect a 3d input innit - added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Machine learning tools\n",
    "import umap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# File load and save imports\n",
    "from utils import *\n",
    "import segypy \n",
    "import pickle\n",
    "\n",
    "# widget imports\n",
    "from ipywidgets import interact, VBox, HBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Tensor board\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# live loss plots\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     12,
     21,
     34,
     51,
     62,
     63,
     105,
     111,
     121,
     131,
     146,
     151,
     166,
     194
    ]
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "def load_seismic(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    seis, header, trace_headers = segypy.readSegy(filename)\n",
    "    amplitude = seis.reshape(header['ns'], inl.size, crl.size)\n",
    "    lagtime = trace_headers['LagTimeA'][0]*-1\n",
    "    twt = np.arange(lagtime, header['dt']/1e3*header['ns']+lagtime, header['dt']/1e3)\n",
    "    return amplitude, twt\n",
    "\n",
    "def load_horizon(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    hrz = np.recfromtxt(filename, names=['il','xl','z'])\n",
    "    horizon = np.zeros((len(inl), len(crl)))\n",
    "    for i, idx in enumerate(inl):\n",
    "        for j, xdx in enumerate(crl):\n",
    "            time = hrz['z'][np.where((hrz['il']== idx) & (hrz['xl'] == xdx))]\n",
    "            if len(time) == 1:\n",
    "                horizon[i, j] = time \n",
    "\n",
    "    return horizon\n",
    "\n",
    "def interpolate_horizon(horizon):\n",
    "    points = []\n",
    "    wanted = []\n",
    "    for i in range(horizon.shape[0]):\n",
    "        for j in range(horizon.shape[1]):\n",
    "            if horizon[i, j] != 0.:\n",
    "                points.append([i, j, horizon[i, j]])\n",
    "            else:\n",
    "                wanted.append([i, j])\n",
    "    \n",
    "    points = np.array(points)\n",
    "    zs2 = scipy.interpolate.griddata(points[:, 0:2], points[:, 2], wanted, method=\"cubic\")\n",
    "    for p, val in zip(wanted, zs2):\n",
    "        horizon[p[0], p[1]] = val\n",
    "    \n",
    "    return horizon\n",
    "\n",
    "def flatten_on_horizon(amplitude, horizon, twt, top_add=12, below_add=52):\n",
    "    traces = np.zeros((horizon.shape[0], horizon.shape[1], top_add+below_add))\n",
    "    for i in range(horizon.shape[0]):\n",
    "        hrz_idx = [np.abs(twt-val).argmin() for val in horizon[i, :]]\n",
    "        for j in range(horizon.shape[1]):\n",
    "            traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "\n",
    "    return traces\n",
    "\n",
    "\n",
    "# VAE functions\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_size, shape_in):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "#         print('\\nINNIT:\\nDATA SHAPE:', shape_in)\n",
    "        # Architecture paramaters\n",
    "        shape = shape_in[-1] # /2 as will be split into near and far channels\n",
    "#         print('dimension assumed after split:', shape)\n",
    "        assert shape%4 == 0, 'input for VAE must be factor of 4'\n",
    "        reductions = [0.5, 0.5, 0.5]  # specify reduction factor of each convolution\n",
    "        self.last_conv_channels = 34 # number of channels after last convolution\n",
    "        \n",
    "        # find the resultant dimension post convolutional layers\n",
    "        post_conv = self.post_conv_dim(shape, reductions, self.last_conv_channels)\n",
    "        self.linear_dimension = post_conv * self.last_conv_channels\n",
    "        \n",
    "#         print('Reductions: {}, Number of Channels on last convultion: {}'.format(reductions, self.last_conv_channels))\n",
    "#         print('Post Conv Dim:', post_conv)\n",
    "#         print('Input * reductions * channels = Lin dimension:', self.linear_dimension)\n",
    "#         print('\\n')\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv1d(2, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(32, self.last_conv_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(self.linear_dimension, 128)\n",
    "\n",
    "        # Latent space\n",
    "        self.fc21 = nn.Linear(128, hidden_size)\n",
    "        self.fc22 = nn.Linear(128, hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(hidden_size, 128)\n",
    "        self.fc4 = nn.Linear(128, self.linear_dimension)\n",
    "        self.deconv1 = nn.ConvTranspose1d(self.last_conv_channels, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv1d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def post_conv_dim(self, in_shape, conv_reductions, last_conv_channels):\n",
    "        \"\"\" Calculates the resultant dimension from convolutions\"\"\"\n",
    "        for i in conv_reductions:\n",
    "            in_shape = int(np.ceil(in_shape*i)) # calc the resultant size from each conv\n",
    "        return in_shape    \n",
    "\n",
    "    def encode(self, x):\n",
    "#         print('in encode, shape:', x.shape)\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        h1 = self.relu(self.fc1(out))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            if mu.is_cuda:\n",
    "                eps = eps.cuda()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        out = self.relu(self.fc4(h3))\n",
    "#         print('in decode, shape before conv(expect linear):', out.shape)\n",
    "        out = out.view(out.size(0), self.last_conv_channels, int(self.linear_dimension/self.last_conv_channels))\n",
    "#         print('in decode, after reshape for conv:', out.shape)\n",
    "        out = self.relu(self.deconv1(out))\n",
    "#         print('in decode, after conv1:', out.shape)\n",
    "        out = self.relu(self.deconv2(out))\n",
    "#         print('in decode, after conv2:', out.shape)\n",
    "        out = self.relu(self.deconv3(out))\n",
    "        out = self.conv5(out)\n",
    "#         print('in decode, end shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z\n",
    "    \n",
    "def loss_function(recon_x, x, mu, logvar, window_size):\n",
    "    criterion_mse = nn.MSELoss(size_average=False)\n",
    "#     print('in loss func, window_size:', window_size)\n",
    "#     print('in loss func, x shape:', x.shape, recon_x.shape)\n",
    "    MSE = criterion_mse(recon_x.view(-1, 2, window_size), x.view(-1, 2, window_size))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD    \n",
    "    \n",
    "# Function to perform one epoch of training\n",
    "def train(epoch, model, optimizer, train_loader, cuda=False, log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "#         print(data.shape)\n",
    "\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, _ = model(data)\n",
    "#         print('In train, data shape:', print(data.shape))\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, window_size=data.shape[-1])\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        optimizer.step()\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                        100. * batch_idx / len(train_loader),\n",
    "#                        loss.item() * data.size(0) / len(train_loader.dataset)))\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss\n",
    "\n",
    "# Function to perform evaluation of data on the model, used for testing\n",
    "def test(epoch, model, test_loader, cuda=False, log_interval=10):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, _ = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar, data.shape[-1]).item() * data.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "# Function to forward_propagate a set of tensors and receive back latent variables and reconstructions\n",
    "def forward_all(model, all_loader, cuda=False):\n",
    "    model.eval()\n",
    "    reconstructions, latents = [], []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(all_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, z = model(data)\n",
    "            reconstructions.append(recon_batch.cpu())\n",
    "            latents.append(z.cpu())\n",
    "    return torch.cat(reconstructions, 0), torch.cat(latents, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegyPY0.57:  readSegy : Trying to read ./data/3d_nearstack.sgy\n",
      "SegyPY0.57:  getSegyHeader : succesfully read ./data/3d_nearstack.sgy\n",
      "SegyPY0.57:  filesize=31438840\n",
      "SegyPY0.57:  bps=    4\n",
      "SegyPY0.57:  nd=7858810\n",
      "SegyPY0.57:  readSegyData : Reading segy data\n",
      "SegyPY0.57:  readSegyData : SEG-Y revision = 0\n",
      "SegyPY0.57:  readSegyData : DataSampleFormat=1(IBM Float)\n",
      "SegyPY0.57:   ns=250\n",
      "SegyPY0.57:  readSegyData : Finished reading segy data\n",
      "SegyPY0.57:  readSegy : Trying to read ./data/3d_farstack.sgy\n",
      "SegyPY0.57:  getSegyHeader : succesfully read ./data/3d_farstack.sgy\n",
      "SegyPY0.57:  filesize=31438840\n",
      "SegyPY0.57:  bps=    4\n",
      "SegyPY0.57:  nd=7858810\n",
      "SegyPY0.57:  readSegyData : Reading segy data\n",
      "SegyPY0.57:  readSegyData : SEG-Y revision = 0\n",
      "SegyPY0.57:  readSegyData : DataSampleFormat=1(IBM Float)\n",
      "SegyPY0.57:   ns=250\n",
      "SegyPY0.57:  readSegyData : Finished reading segy data\n"
     ]
    }
   ],
   "source": [
    "### Client loader\n",
    "dataholder = DataHolder(\"Glitne\", [1300, 1502, 2], [1500, 2002, 2])\n",
    "dataholder.add_near('./data/3d_nearstack.sgy');\n",
    "dataholder.add_far('./data/3d_farstack.sgy');\n",
    "dataholder.add_horizon('./data/Top_Heimdal_subset.txt')\n",
    "dataholder.add_well('well_1', 36, 276//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     1,
     16,
     20,
     31,
     40,
     58,
     65,
     76,
     79,
     93
    ]
   },
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    def __init__(self, field_name, inlines, xlines):\n",
    "        \n",
    "        # User input attributes\n",
    "        self.field_name = field_name\n",
    "        self.inlines = inlines\n",
    "        self.xlines = xlines\n",
    "\n",
    "        # KEY data for processing\n",
    "        self.near = None\n",
    "        self.far = None\n",
    "        self.twt = None\n",
    "        self.horizon = None\n",
    "        \n",
    "        self.wells = {}\n",
    "        \n",
    "    def add_near(self, fname):\n",
    "        self.near, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        self.twt = twt\n",
    "\n",
    "    def add_far(self, fname):\n",
    "        self.far, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        assert (self.twt == twt).all, \"This twt does not match the twt from the previous segy\"\n",
    "        \n",
    "    def add_horizon(self, fname):\n",
    "        self.horizon = interpolate_horizon(load_horizon(fname, inlines=self.inlines, xlines=self.xlines))\n",
    "        \n",
    "    def add_well(self, well_id, well_i, well_x):\n",
    "        self.wells[well_id] = [well_i, well_x]\n",
    "           \n",
    "            \n",
    "class Processor:\n",
    "    def __init__(self, Data):\n",
    "        self.raw = [Data.far, Data.near]\n",
    "        self.twt = Data.twt\n",
    "        self.out = None\n",
    "        \n",
    "        # attributes\n",
    "        self.attributes = {'horizon_raw': Data.horizon}\n",
    "    \n",
    "    def flatten(self, data, top_add=12, below_add=52):\n",
    "        out = []\n",
    "        horizon = self.attributes['horizon_raw']\n",
    "        \n",
    "        # input data = [near(twt, x1, x2),far(twt, x1, x2)]\n",
    "        for amplitude in data:\n",
    "            # create output trace shape for each set in shape: (twt, x1, x2)\n",
    "            traces = np.zeros((top_add+below_add, horizon.shape[0],horizon.shape[1]))\n",
    "            for i in range(horizon.shape[0]):\n",
    "                # find the corresponding index of the horizon in amplitude twt 'domain'\n",
    "                hrz_idx = [np.abs(self.twt-val).argmin() for val in horizon[i, :]]\n",
    "                for j in range(horizon.shape[1]):\n",
    "                    # place the twt's from above_below horizon into 3rd index\n",
    "                    traces[:, i, j] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "            out.append(traces)\n",
    "\n",
    "        return out # list of far and near, flattened amplitudes shape (twt, x1,x2)\n",
    "    \n",
    "    def roll_axis(self, data):\n",
    "        #input should be (twt,x1,x2)\n",
    "        for i in range(len(data)):\n",
    "            data[i] = np.transpose(data[i], (1, 2, 0))\n",
    "        #output (x1,x2,twt)\n",
    "        return data\n",
    "    \n",
    "    def normalise(self, data):\n",
    "        well_i=38\n",
    "        well_x=138\n",
    "        out = []\n",
    "        for i in data:\n",
    "            well_variance = np.mean(np.std(i[well_i - 2:well_i + 1, well_x - 2:well_x + 1], 2))\n",
    "            i /= well_variance\n",
    "            out.append(i)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def to_2d(self, data):\n",
    "        return [i.reshape(-1, data[0].shape[-1]) for i in data]\n",
    "          \n",
    "    def stack_traces(self, data):\n",
    "        return np.stack([data[0], data[1]], axis=1)\n",
    "#         return  np.concatenate([i for i in data], axis=1)\n",
    "    \n",
    "    def run_AVO(self):\n",
    "#         print(self.out[0].shape, self.out[1].shape)\n",
    "        x_avo = self.out[1]\n",
    "        y_avo = self.out[0] - self.out[1]\n",
    "\n",
    "        lin_reg = LinearRegression(fit_intercept=False, normalize=False, copy_X=True, n_jobs=1)\n",
    "        lin_reg.fit(x_avo.reshape(-1, 1), y_avo.reshape(-1, 1))\n",
    "\n",
    "        self.attributes['FF'] = y_avo - lin_reg.coef_ * x_avo\n",
    "        \n",
    "    def condition_attributes(self):\n",
    "        # flatten horizon array\n",
    "        horizon = self.attributes['horizon_raw']\n",
    "        self.attributes['horizon'] = horizon.reshape(horizon.shape[0]*horizon.shape[1])\n",
    "        \n",
    "        # condense fluid factor to min of array\n",
    "        self.attributes['FF'] = np.min(self.attributes['FF'], 1)\n",
    "    \n",
    "    def __call__(self, flatten=False, normalise=False):\n",
    "        self.out = copy.copy(self.raw)\n",
    "        \n",
    "        if flatten[0]:\n",
    "            self.out = self.flatten(self.out, flatten[1], flatten[2])\n",
    "            \n",
    "        self.out = self.roll_axis(self.out)\n",
    "        \n",
    "        if normalise:\n",
    "            self.out = self.normalise(self.out)\n",
    "        \n",
    "        # flatten to 2d (traces, amplitudes)\n",
    "        self.out = self.to_2d(self.out)\n",
    "        \n",
    "        # Find fluid factor, add to attributes\n",
    "        self.FF = self.run_AVO()\n",
    "        \n",
    "        # condition attributes to 1d arrays\n",
    "        self.condition_attributes()\n",
    "        \n",
    "        # Stack the traces for output\n",
    "        self.out = self.stack_traces(self.out)\n",
    "        print('Processor has made an output with shape: ', self.out.shape)\n",
    "        \n",
    "\n",
    "        return [self.out, self.attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     1,
     17,
     18,
     47,
     77
    ]
   },
   "outputs": [],
   "source": [
    "class ModelAgent:\n",
    "    def __init__(self, data):\n",
    "        self.input = data[0]\n",
    "        self.attributes = data[1]\n",
    "        self.embedding = None\n",
    "        self.input_dimension= self.input.shape[-1]\n",
    "        \n",
    "        # for logging\n",
    "#         today = datetime.date.today()\n",
    "#         self.path = './runs/{}'.format(today)\n",
    "#         if not os.path.exists(self.path):\n",
    "#             os.mkdir(self.path)\n",
    "#             print(\"Directory \" , self.path ,  \" For Logs Created \")\n",
    "        \n",
    "        print(\"ModelAgent initialised\")\n",
    "        \n",
    "\n",
    "class UMAP(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "        self.name = 'UMAP'\n",
    "        \n",
    "    def concat(self):\n",
    "#         self.input = np.concatenate([self.input[0],self.input[1]], 1)\n",
    "\n",
    "        self.input = self.input.reshape(-1, 2*self.input_dimension)\n",
    "        print('to enter UMAP:', self.input.shape)\n",
    "\n",
    "\n",
    "    def reduce(self, n_neighbors = 50, min_dist=0.001):\n",
    "        # Directory for logging runs\n",
    "#         now = datetime.datetime.now().strftime(\"%I-%M-%S-%p\")\n",
    "#         self.path = self.path + '/{}/'.format(now, self.name)\n",
    "\n",
    "        self.concat() # concat near_far into 1 dim\n",
    "\n",
    "        embedding = umap.UMAP(n_neighbors=n_neighbors,\n",
    "                      min_dist=min_dist,\n",
    "                      metric='correlation', \n",
    "                               verbose=False,\n",
    "                            random_state=42).fit_transform(self.input)\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        print(\"UMAP 2-D representation complete\")\n",
    "        \n",
    "    \n",
    "class VAE_model(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "        self.name = 'VAE'\n",
    "\n",
    "    \n",
    "    def create_dataloader(self, batch_size=32):\n",
    "        # create torch tensor\n",
    "        assert self.input.shape[1] == 2, 'expecting a 3D input'\n",
    "        X = torch.from_numpy(self.input).float()\n",
    "        # split the concatenated input back into two arrays\n",
    "#         X = torch.from_numpy(np.stack(np.split(self.input, 2, axis=1), 1)).float()\n",
    "        \n",
    "        # Create a stacked representation and a zero tensor so we can use the standard Pytorch TensorDataset\n",
    "        y = torch.from_numpy(np.zeros((X.shape[0], 1))).float()\n",
    "        \n",
    "        split = ShuffleSplit(n_splits=1, test_size=0.5)\n",
    "        for train_index, test_index in split.split(X):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        train_dset = TensorDataset(X_train, y_train)\n",
    "        test_dset = TensorDataset(X_test, y_test)\n",
    "        all_dset = TensorDataset(X, y)\n",
    "\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        self.all_loader = torch.utils.data.DataLoader(all_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        \n",
    "    def train_vae(self, cuda=False, epochs=5, hidden_size=8, lr=1e-2):\n",
    "        set_seed(42)  # Set the random seed\n",
    "        self.model = VAE(hidden_size, self.input.shape)  # Inititalize the model\n",
    "\n",
    "        # Create a gradient descent optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "        \n",
    "        liveloss = PlotLosses()\n",
    "        liveloss.skip_first = 0\n",
    "        liveloss.figsize = (16,10) #, fig_path=self.path\n",
    "        liveloss.fig_path = './'\n",
    "        \n",
    "        # Start training loop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tl = train(epoch, self.model, optimizer, self.train_loader, cuda=False)  # Train model on train dataset\n",
    "            testl = test(epoch, self.model, self.test_loader, cuda=False)  # Validate model on test dataset\n",
    "#             %matplotlib inline\n",
    "            logs = {}\n",
    "            logs[ '' + 'ELBO'] = tl\n",
    "            logs[ 'val_' + 'ELBO'] = testl\n",
    "            liveloss.update(logs)\n",
    "            liveloss.draw()      \n",
    "\n",
    "    def run_vae(self):\n",
    "        _, self.zs = forward_all(self.model, self.all_loader, cuda=False)\n",
    "        \n",
    "    def vae_umap(self, umap_neighbours=50, umap_dist=0.001):\n",
    "        print('\\nVAE->UMAP representation initialised\\n')\n",
    "        transformer = umap.UMAP(n_neighbors=umap_neighbours,\n",
    "                                min_dist=umap_dist,\n",
    "                                metric='correlation', verbose=True).fit(self.zs.numpy())\n",
    "        embedding = transformer.transform(self.zs.numpy())\n",
    "        print(\"\\n\\nVAE -> 2-D UMAP representation complete\\n\")\n",
    "        return embedding\n",
    "    \n",
    "    def reduce(self, epochs, hidden_size, lr, umap_neighbours, umap_dist):\n",
    "        if hidden_size < 2: raise Exception('Please use hidden size > 1')\n",
    "\n",
    "        # Directory for logging runs\n",
    "        now = datetime.datetime.now().strftime(\"%I-%M-%S-%p\")\n",
    "        self.path = self.path + '/{}/'.format(now, self.name)\n",
    "        \n",
    "        # TODO create text file detailing all hyper parameters.\n",
    "        \n",
    "        if not os.path.exists(self.path):\n",
    "            os.mkdir(self.path)\n",
    "            print(\"Directory \" , self.path ,  \" For Logs Created \")\n",
    "\n",
    "\n",
    "        self.create_dataloader()\n",
    "        self.train_vae(epochs=epochs, hidden_size=hidden_size, lr=lr)\n",
    "        self.run_vae()\n",
    "        \n",
    "        \n",
    "        # Find 2-D embedding\n",
    "        if hidden_size > 2:\n",
    "            self.embedding = self.vae_umap(umap_dist=umap_dist, umap_neighbours=umap_neighbours)\n",
    "        elif hidden_size == 2:\n",
    "            self.embedding = self.zs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def PlotAgent(model, attr):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.set(xlabel='Latent Variable 1', ylabel='Latent Variable 2',\n",
    "               title='Model used: {}, Trace Attribute: {}'.format(model.name, attr),\n",
    "               aspect='equal')\n",
    "        s = ax.scatter(model.embedding[:, 0], model.embedding[:, 1], s=1.0, c=model.attributes[attr])\n",
    "        c = plt.colorbar(s, shrink=0.7, orientation='vertical')\n",
    "        c.set_label(label=attr, rotation=90, labelpad=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor has made an output with shape:  (25351, 2, 250)\n",
      "ModelAgent initialised\n",
      "to enter UMAP: (25351, 500)\n"
     ]
    }
   ],
   "source": [
    "# sanity check to see everything is working\n",
    "processor = Processor(dataholder)\n",
    "input12341 = processor([False, 12, 52], normalise=True)\n",
    "UMAP_a = UMAP(input12341)\n",
    "UMAP_a.reduce(n_neighbors=50)\n",
    "PlotAgent(UMAP_a, 'FF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPAND WINDOW OVER HORIZON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key facts on data set:\n",
    "mn = dataholder.horizon.min()\n",
    "mx = dataholder.horizon.max()\n",
    "mnid = np.abs(dataholder.twt-mn).argmin()\n",
    "mxid = np.abs(dataholder.twt-mx).argmin()\n",
    "\n",
    "print('Dataset depth in twt = \\t250')\n",
    "print('Horizon min max index:\\t', mnid, ':', mxid)\n",
    "print('Horizon span:\\t\\t', mxid - mnid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processor\n",
    "processor = Processor(dataholder)\n",
    "\n",
    "inputs = []\n",
    "windows = np.arange(2,88, 10)\n",
    "print(windows)\n",
    "for i in windows:\n",
    "    inputs.append(processor([True, i, i], normalise=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UMAP RUN\n",
    "for count,  i in enumerate(inputs):\n",
    "    UMAP_a = UMAP(i)\n",
    "    UMAP_a.reduce(n_neighbors=50)\n",
    "    print('window size=', 2* gaps[count])\n",
    "    PlotAgent(UMAP_a, 'FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCROLL WINDOW OVER HORIZON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processor\n",
    "processor = Processor(dataholder)\n",
    "\n",
    "inputs = []\n",
    "window = 80\n",
    "\n",
    "for i in range (0,9):\n",
    "    print('below add:', i*10, 'above add', window-i*10)\n",
    "    inputs.append(processor([True, window-i*10, i*10], normalise=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UMAP RUN\n",
    "for count,  i in enumerate(inputs):\n",
    "    UMAP_a = UMAP(i)\n",
    "    UMAP_a.reduce(n_neighbors=50)\n",
    "    print('window position =', count*10)\n",
    "    PlotAgent(UMAP_a, 'FF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processor\n",
    "processor = Processor(dataholder)\n",
    "input123 = processor([False, 0, 0], normalise=True)\n",
    "\n",
    "window_half = np.arange(15,90, 10)\n",
    "mid = 147\n",
    "print(window_half)\n",
    "\n",
    "man_inputs = []\n",
    "\n",
    "for i in window_half:\n",
    "    item = input123[0][:,:,mid-i:mid+i] # create the manual data window for each window half value\n",
    "    man_inputs.append(item)\n",
    "    print(item.shape)\n",
    "    \n",
    "    \n",
    "### UMAP RUN\n",
    "for count,  i in enumerate(window_half):\n",
    "    input123[0] = man_inputs[count] # manually asign relevant data to the input object\n",
    "    UMAP_a = UMAP(input123)\n",
    "    UMAP_a.reduce(n_neighbors=50)\n",
    "    print('window_half size=', window_half[count])\n",
    "    PlotAgent(UMAP_a, 'FF')\n",
    "    PlotAgent(UMAP_a, 'horizon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_stack_amplitudes, twt = load_seismic('./data/3d_nearstack.sgy', inlines=[1300, 1502, 2], xlines=[1500, 2002, 2])\n",
    "far_stack_amplitudes, _ = load_seismic('./data/3d_farstack.sgy', inlines=[1300, 1502, 2], xlines=[1500, 2002, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = load_horizon('./data/Top_Heimdal_subset.txt', inlines=[1300, 1502, 2], xlines=[1500, 2002, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_horizon = interpolate_horizon(horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_traces = flatten_on_horizon(near_stack_amplitudes, horizon, twt, top_add=12, below_add=52)\n",
    "far_traces = flatten_on_horizon(far_stack_amplitudes, horizon, twt, top_add=12, below_add=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_i, well_x = 36, 276//2\n",
    "well_variance_near = np.mean(np.std(near_traces[well_i-2:well_i+1, well_x-2:well_x+1], 2))\n",
    "well_variance_far = np.mean(np.std(far_traces[well_i-2:well_i+1, well_x-2:well_x+1], 2))\n",
    "\n",
    "near_traces /= well_variance_near\n",
    "far_traces /= well_variance_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = far_traces - near_traces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_traces_emb = near_traces.reshape(-1, 64)\n",
    "far_traces_emb = far_traces.reshape(-1, 64)\n",
    "diff_traces_emb = diff.reshape(-1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.concatenate([far_traces_emb, near_traces_emb], 1)\n",
    "three_features = np.concatenate([far_traces_emb, near_traces_emb, diff_traces_emb], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#FF\n",
    "x_avo = near_traces_emb\n",
    "y_avo = far_traces_emb-near_traces_emb\n",
    "lin_reg = LinearRegression(fit_intercept=False, normalize=False, copy_X=True, n_jobs=1)\n",
    "lin_reg.fit(x_avo.reshape(-1, 1), y_avo.reshape(-1, 1))\n",
    "FF = y_avo-lin_reg.coef_*x_avo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umapit(it):\n",
    "    plot = umap.UMAP(n_neighbors=50,\n",
    "                          min_dist=0.001,\n",
    "                          metric='correlation', \n",
    "                                   verbose=True,\n",
    "                                random_state=42).fit_transform(it)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.set(xlabel='Latent Variable 1', ylabel='Latent Variable 2',\n",
    "           aspect='equal')\n",
    "    sc = ax.scatter(plot[:, 0], plot[:, 1], s=1.0, c=np.min(FF, 1))\n",
    "    c = plt.colorbar(sc, shrink=0.7, orientation='vertical')\n",
    "    c.set_label(label= 'ada', rotation=90, labelpad=10)\n",
    "    plt.show()\n",
    "    \n",
    "    return embedding_stack_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = umapit(three_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
