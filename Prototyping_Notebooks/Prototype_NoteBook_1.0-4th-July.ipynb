{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Machine learning tools\n",
    "import umap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# File load and save imports\n",
    "from utils import *\n",
    "import segypy \n",
    "import pickle\n",
    "\n",
    "# widget imports\n",
    "from ipywidgets import interact, VBox, HBox\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Tensor board\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# live loss plots\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy paste \"utils.py\" because it wont load/import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     1,
     12,
     21,
     34,
     51,
     62,
     151,
     166,
     194,
     210
    ]
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "def load_seismic(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    seis, header, trace_headers = segypy.readSegy(filename)\n",
    "    amplitude = seis.reshape(header['ns'], inl.size, crl.size)\n",
    "    lagtime = trace_headers['LagTimeA'][0]*-1\n",
    "    twt = np.arange(lagtime, header['dt']/1e3*header['ns']+lagtime, header['dt']/1e3)\n",
    "    return amplitude, twt\n",
    "\n",
    "def load_horizon(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    hrz = np.recfromtxt(filename, names=['il','xl','z'])\n",
    "    horizon = np.zeros((len(inl), len(crl)))\n",
    "    for i, idx in enumerate(inl):\n",
    "        for j, xdx in enumerate(crl):\n",
    "            time = hrz['z'][np.where((hrz['il']== idx) & (hrz['xl'] == xdx))]\n",
    "            if len(time) == 1:\n",
    "                horizon[i, j] = time \n",
    "\n",
    "    return horizon\n",
    "\n",
    "def interpolate_horizon(horizon):\n",
    "    points = []\n",
    "    wanted = []\n",
    "    for i in range(horizon.shape[0]):\n",
    "        for j in range(horizon.shape[1]):\n",
    "            if horizon[i, j] != 0.:\n",
    "                points.append([i, j, horizon[i, j]])\n",
    "            else:\n",
    "                wanted.append([i, j])\n",
    "    \n",
    "    points = np.array(points)\n",
    "    zs2 = scipy.interpolate.griddata(points[:, 0:2], points[:, 2], wanted, method=\"cubic\")\n",
    "    for p, val in zip(wanted, zs2):\n",
    "        horizon[p[0], p[1]] = val\n",
    "    \n",
    "    return horizon\n",
    "\n",
    "def flatten_on_horizon(amplitude, horizon, twt, top_add=12, below_add=52):\n",
    "    traces = np.zeros((horizon.shape[0], horizon.shape[1], top_add+below_add))\n",
    "    for i in range(horizon.shape[0]):\n",
    "        hrz_idx = [np.abs(twt-val).argmin() for val in horizon[i, :]]\n",
    "        for j in range(horizon.shape[1]):\n",
    "            traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "\n",
    "    return traces\n",
    "\n",
    "\n",
    "# VAE functions\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_size, shape_in):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "#         print('\\nINNIT:\\nDATA SHAPE:', shape_in)\n",
    "        # Architecture paramaters\n",
    "        shape = shape_in[-1]/2 # /2 as will be split into near and far channels\n",
    "#         print('dimension assumed after split:', shape)\n",
    "        assert shape%4 == 0, 'input for VAE must be factor of 4'\n",
    "        reductions = [0.5, 0.5, 0.5]  # specify reduction factor of each convolution\n",
    "        self.last_conv_channels = 34 # number of channels after last convolution\n",
    "        \n",
    "        # find the resultant dimension post convolutional layers\n",
    "        post_conv = self.post_conv_dim(shape, reductions, self.last_conv_channels)\n",
    "        self.linear_dimension = post_conv * self.last_conv_channels\n",
    "        \n",
    "#         print('Reductions: {}, Number of Channels on last convultion: {}'.format(reductions, self.last_conv_channels))\n",
    "#         print('Post Conv Dim:', post_conv)\n",
    "#         print('Input * reductions * channels = Lin dimension:', self.linear_dimension)\n",
    "#         print('\\n')\n",
    "        \n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv1d(2, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(32, self.last_conv_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(self.linear_dimension, 128)\n",
    "\n",
    "        # Latent space\n",
    "        self.fc21 = nn.Linear(128, hidden_size)\n",
    "        self.fc22 = nn.Linear(128, hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(hidden_size, 128)\n",
    "        self.fc4 = nn.Linear(128, self.linear_dimension)\n",
    "        self.deconv1 = nn.ConvTranspose1d(self.last_conv_channels, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv1d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def post_conv_dim(self, in_shape, conv_reductions, last_conv_channels):\n",
    "        \"\"\" Calculates the resultant dimension from convolutions\"\"\"\n",
    "        for i in conv_reductions:\n",
    "            in_shape = int(np.ceil(in_shape*i)) # calc the resultant size from each conv\n",
    "        return in_shape    \n",
    "\n",
    "    def encode(self, x):\n",
    "#         print('in encode, shape:', x.shape)\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        h1 = self.relu(self.fc1(out))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            if mu.is_cuda:\n",
    "                eps = eps.cuda()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        out = self.relu(self.fc4(h3))\n",
    "#         print('in decode, shape before conv(expect linear):', out.shape)\n",
    "        out = out.view(out.size(0), self.last_conv_channels, int(self.linear_dimension/self.last_conv_channels))\n",
    "#         print('in decode, after reshape for conv:', out.shape)\n",
    "        out = self.relu(self.deconv1(out))\n",
    "#         print('in decode, after conv1:', out.shape)\n",
    "        out = self.relu(self.deconv2(out))\n",
    "#         print('in decode, after conv2:', out.shape)\n",
    "        out = self.relu(self.deconv3(out))\n",
    "        out = self.conv5(out)\n",
    "#         print('in decode, end shape:', out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z\n",
    "    \n",
    "def loss_function(recon_x, x, mu, logvar, window_size):\n",
    "    criterion_mse = nn.MSELoss(size_average=False)\n",
    "#     print('in loss func, window_size:', window_size)\n",
    "#     print('in loss func, x shape:', x.shape, recon_x.shape)\n",
    "    MSE = criterion_mse(recon_x.view(-1, 2, window_size), x.view(-1, 2, window_size))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD    \n",
    "    \n",
    "# Function to perform one epoch of training\n",
    "def train(epoch, model, optimizer, train_loader, cuda=False, log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "#         print(data.shape)\n",
    "\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, _ = model(data)\n",
    "#         print('In train, data shape:', print(data.shape))\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, window_size=data.shape[-1])\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        optimizer.step()\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                        100. * batch_idx / len(train_loader),\n",
    "#                        loss.item() * data.size(0) / len(train_loader.dataset)))\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss\n",
    "\n",
    "# Function to perform evaluation of data on the model, used for testing\n",
    "def test(epoch, model, test_loader, cuda=False, log_interval=10):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, _ = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar, data.shape[-1]).item() * data.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "# Function to forward_propagate a set of tensors and receive back latent variables and reconstructions\n",
    "def forward_all(model, all_loader, cuda=False):\n",
    "    model.eval()\n",
    "    reconstructions, latents = [], []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(all_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, z = model(data)\n",
    "            reconstructions.append(recon_batch.cpu())\n",
    "            latents.append(z.cpu())\n",
    "    return torch.cat(reconstructions, 0), torch.cat(latents, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size torch.Size([100, 2, 104])\n",
      "woops\n"
     ]
    }
   ],
   "source": [
    "# testing VAE architecture\n",
    "indim  = 100\n",
    "\n",
    "x = torch.randn((100, 2, indim))\n",
    "\n",
    "\n",
    "model = VAE(8, (100,1, indim*2))\n",
    "y = model(x)\n",
    "\n",
    "\n",
    "print(\"output size\", y[0].shape)\n",
    "\n",
    "if 210%4 != 0:\n",
    "    print('woops')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 'BACKEND':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DataHolder + Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     16,
     20,
     24,
     27,
     31,
     40,
     54,
     65,
     68,
     72,
     82,
     90
    ]
   },
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    def __init__(self, field_name, inlines, xlines):\n",
    "        \n",
    "        # User input attributes\n",
    "        self.field_name = field_name\n",
    "        self.inlines = inlines\n",
    "        self.xlines = xlines\n",
    "\n",
    "        # KEY data for processing\n",
    "        self.near = None\n",
    "        self.far = None\n",
    "        self.twt = None\n",
    "        self.horizon = None\n",
    "        \n",
    "        self.wells = {}\n",
    "        \n",
    "    def add_near(self, fname):\n",
    "        self.near, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        self.twt = twt\n",
    "\n",
    "    def add_far(self, fname):\n",
    "        self.far, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        assert (self.twt == twt).all, \"This twt does not match the twt from the previous segy\"\n",
    "        \n",
    "    def add_horizon(self, fname):\n",
    "        self.horizon = interpolate_horizon(load_horizon(fname, inlines=self.inlines, xlines=self.xlines))\n",
    "        \n",
    "    def add_well(self, well_id, well_i, well_x):\n",
    "        self.wells[well_id] = [well_i, well_x]\n",
    "           \n",
    "            \n",
    "class Processor:\n",
    "    def __init__(self, Data):\n",
    "        self.raw = [Data.near, Data.far]\n",
    "        self.twt = Data.twt\n",
    "        self.out = None\n",
    "        \n",
    "        # attributes\n",
    "        self.attributes = {'horizon': Data.horizon}\n",
    "    \n",
    "    def flatten(self, data, top_add=12, below_add=52):\n",
    "        out = []\n",
    "        horizon = self.attributes['horizon']\n",
    "        \n",
    "        for amplitude in data:\n",
    "            traces = np.zeros((horizon.shape[0],horizon.shape[1], top_add+below_add))\n",
    "            for i in range(horizon.shape[0]):\n",
    "                hrz_idx = [np.abs(self.twt-val).argmin() for val in horizon[i, :]] \n",
    "                for j in range(horizon.shape[1]):\n",
    "                    traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "            out.append(traces)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def normalise(self, data):\n",
    "        well_i=38\n",
    "        well_x=138\n",
    "        out = []\n",
    "        for i in data:\n",
    "            well_variance = np.mean(np.std(i[well_i - 2:well_i + 1, well_x - 2:well_x + 1], 2))\n",
    "            i /= well_variance\n",
    "            out.append(i)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def to_2d(self, data):\n",
    "        return [i.reshape(-1, data[0].shape[-1]) for i in data]\n",
    "          \n",
    "    def stack_traces(self, data):\n",
    "#       return np.stack([data[0], data[0]], axis=1)\n",
    "        return  np.concatenate([i for i in data], 1)\n",
    "    \n",
    "    def run_AVO(self):\n",
    "#         print(self.out[0].shape, self.out[1].shape)\n",
    "        x_avo = self.out[0]\n",
    "        y_avo = self.out[1] - self.out[0]\n",
    "\n",
    "        lin_reg = LinearRegression(fit_intercept=False, normalize=False, copy_X=True, n_jobs=1)\n",
    "        lin_reg.fit(x_avo.reshape(-1, 1), y_avo.reshape(-1, 1))\n",
    "\n",
    "        self.attributes['FF'] = y_avo - lin_reg.coef_ * x_avo\n",
    "        \n",
    "    def condition_attributes(self):\n",
    "        # flatten horizon array\n",
    "        horizon = self.attributes['horizon']\n",
    "        self.attributes['horizon'] = horizon.reshape(horizon.shape[0]*horizon.shape[1])\n",
    "        \n",
    "        # condense fluid factor to min of array\n",
    "        self.attributes['FF'] = np.min(self.attributes['FF'], 1)\n",
    "    \n",
    "    def __call__(self, flatten=False, normalise=False):\n",
    "        self.out = copy.copy(self.raw)\n",
    "        \n",
    "        if flatten[0]:\n",
    "            self.out = self.flatten(self.out, flatten[1], flatten[2])\n",
    "        else: # perform the same axis manipulation as flattening method\n",
    "            self.out[0] = self.out[0].reshape([self.out[0].shape[1], self.out[0].shape[2], self.out[0].shape[0]])\n",
    "            self.out[1] = self.out[1].reshape([self.out[1].shape[1], self.out[1].shape[2], self.out[1].shape[0]])\n",
    "     \n",
    "        if normalise:\n",
    "            self.out = self.normalise(self.out)\n",
    "        \n",
    "        # flatten to 2d (traces, amplitudes)\n",
    "        self.out = self.to_2d(self.out)\n",
    "        \n",
    "        # Find fluid factor, add to attributes\n",
    "        self.FF = self.run_AVO()\n",
    "        \n",
    "        # Stack the traces for output\n",
    "        self.out = self.stack_traces(self.out)\n",
    "        print(self.out.shape)\n",
    "        \n",
    "        # condition attributes to 1d arrays\n",
    "        self.condition_attributes()\n",
    "        \n",
    "        return [self.out, self.attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ModelAgent -> UMAP + VAE + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     1,
     17,
     18,
     22,
     40,
     41,
     46,
     69
    ]
   },
   "outputs": [],
   "source": [
    "class ModelAgent:\n",
    "    def __init__(self, data):\n",
    "        self.input = data[0]\n",
    "        self.attributes = data[1]\n",
    "        self.embedding = None\n",
    "        self.input_dimension= self.input.shape[-1]\n",
    "        \n",
    "        today = datetime.date.today()\n",
    "        self.path = './runs/{}'.format(today)\n",
    "        if not os.path.exists(self.path):\n",
    "            os.mkdir(self.path)\n",
    "            print(\"Directory \" , self.path ,  \" For Logs Created \")\n",
    "        \n",
    "        \n",
    "        print(\"ModelAgent initialised\")\n",
    "        \n",
    "\n",
    "class UMAP(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "        self.name = 'UMAP'\n",
    "\n",
    "    def reduce(self, n_neighbors = 50, min_dist=0.001):\n",
    "        # Directory for logging runs\n",
    "        now = datetime.datetime.now().strftime(\"%I-%M-%S-%p\")\n",
    "        self.path = self.path + '/{}/'.format(now, self.name)\n",
    "\n",
    "        data = self.input\n",
    "\n",
    "        embedding = umap.UMAP(n_neighbors=n_neighbors,\n",
    "                      min_dist=min_dist,\n",
    "                      metric='correlation', \n",
    "                               verbose=True,\n",
    "                            random_state=42).fit_transform(self.input)\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        print(\"UMAP 2-D representation complete\")\n",
    "        \n",
    "    \n",
    "class VAE_model(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "        self.name = 'VAE'\n",
    "\n",
    "    \n",
    "    def create_dataloader(self, batch_size=32):\n",
    "        # create torch tensor\n",
    "#         X = torch.from_numpy(self.input).float()\n",
    "        # split the concatenated input back into two arrays\n",
    "        X = torch.from_numpy(np.stack(np.split(self.input, 2, axis=1), 1)).float()\n",
    "        \n",
    "        # Create a stacked representation and a zero tensor so we can use the standard Pytorch TensorDataset\n",
    "        y = torch.from_numpy(np.zeros((X.shape[0], 1))).float()\n",
    "        \n",
    "        split = ShuffleSplit(n_splits=1, test_size=0.5)\n",
    "        for train_index, test_index in split.split(X):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        train_dset = TensorDataset(X_train, y_train)\n",
    "        test_dset = TensorDataset(X_test, y_test)\n",
    "        all_dset = TensorDataset(X, y)\n",
    "\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        self.all_loader = torch.utils.data.DataLoader(all_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        \n",
    "    def train_vae(self, cuda=False, epochs=5, hidden_size=8, lr=1e-2):\n",
    "        set_seed(42)  # Set the random seed\n",
    "        self.model = VAE(hidden_size, self.input.shape)  # Inititalize the model\n",
    "\n",
    "        # Create a gradient descent optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "        \n",
    "        liveloss = PlotLosses()\n",
    "        liveloss.skip_first = 0\n",
    "        liveloss.figsize = (16,10) #, fig_path=self.path\n",
    "        liveloss.fig_path = './'\n",
    "        \n",
    "        # Start training loop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tl = train(epoch, self.model, optimizer, self.train_loader, cuda=False)  # Train model on train dataset\n",
    "            testl = test(epoch, self.model, self.test_loader, cuda=False)  # Validate model on test dataset\n",
    "#             %matplotlib inline\n",
    "            logs = {}\n",
    "            logs[ '' + 'ELBO'] = tl\n",
    "            logs[ 'val_' + 'ELBO'] = testl\n",
    "            liveloss.update(logs)\n",
    "            liveloss.draw()      \n",
    "\n",
    "    def run_vae(self):\n",
    "        _, self.zs = forward_all(self.model, self.all_loader, cuda=False)\n",
    "        \n",
    "    def vae_umap(self, umap_neighbours=50, umap_dist=0.001):\n",
    "        print('\\nVAE->UMAP representation initialised\\n')\n",
    "        transformer = umap.UMAP(n_neighbors=umap_neighbours,\n",
    "                                min_dist=umap_dist,\n",
    "                                metric='correlation', verbose=True).fit(self.zs.numpy())\n",
    "        embedding = transformer.transform(self.zs.numpy())\n",
    "        print(\"\\n\\nVAE -> 2-D UMAP representation complete\\n\")\n",
    "        return embedding\n",
    "    \n",
    "    def reduce(self, epochs, hidden_size, lr, umap_neighbours, umap_dist):\n",
    "        if hidden_size < 2: raise Exception('Please use hidden size > 1')\n",
    "\n",
    "        # Directory for logging runs\n",
    "        now = datetime.datetime.now().strftime(\"%I-%M-%S-%p\")\n",
    "        self.path = self.path + '/{}/'.format(now, self.name)\n",
    "        \n",
    "        # TODO create text file detailing all hyper parameters.\n",
    "        \n",
    "        if not os.path.exists(self.path):\n",
    "            os.mkdir(self.path)\n",
    "            print(\"Directory \" , self.path ,  \" For Logs Created \")\n",
    "\n",
    "\n",
    "        self.create_dataloader()\n",
    "        self.train_vae(epochs=epochs, hidden_size=hidden_size, lr=lr)\n",
    "        self.run_vae()\n",
    "        \n",
    "        \n",
    "        # Find 2-D embedding\n",
    "        if hidden_size > 2:\n",
    "            self.embedding = self.vae_umap(umap_dist=umap_dist, umap_neighbours=umap_neighbours)\n",
    "        elif hidden_size == 2:\n",
    "            self.embedding = self.zs.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 PlottingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def PlotAgent(model, attr):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.set(xlabel='Latent Variable 1', ylabel='Latent Variable 2',\n",
    "               title='Model used: {}, Trace Attribute: {}'.format(model.name, attr),\n",
    "               aspect='equal')\n",
    "        s = ax.scatter(model.embedding[:, 0], model.embedding[:, 1], s=1.0, c=model.attributes[attr])\n",
    "        c = plt.colorbar(s, shrink=0.7, orientation='vertical')\n",
    "        c.set_label(label=attr, rotation=90, labelpad=10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BASIS API  example usage (for manual testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Client loader\n",
    "# dataholder = DataHolder(\"Glitne\", [1300, 1502, 2], [1500, 2002, 2])\n",
    "# dataholder.add_near('./data/3d_nearstack.sgy');\n",
    "# dataholder.add_far('./data/3d_farstack.sgy');\n",
    "# dataholder.add_horizon('./data/Top_Heimdal_subset.txt')\n",
    "# dataholder.add_well('well_1', 36, 276//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Processor\n",
    "# processor = Processor(dataholder)\n",
    "\n",
    "# Input1 = processor([True, 12,52], normalise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### UMAP RUN\n",
    "# UMAP_a = UMAP(Input1)\n",
    "# UMAP_a1 = UMAP_a.reduce(n_neighbors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### VAE RUN\n",
    "# VAE_1 = VAE_model(Input1)\n",
    "\n",
    "# VAE_1.reduce(epochs=4, hidden_size=8, lr=1e-2, umap_neighbours=50, umap_dist=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### make a plot\n",
    "\n",
    "# PlotAgent(VAE_1, 'horizon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WIDGET INTERFACE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Widget Definitions\n",
    "\n",
    "# Data Loading:\n",
    "data_files_title = widgets.HTML(\n",
    "    value=\"<b>File Pathnames:<b>\",\n",
    ")\n",
    "\n",
    "near_text = widgets.Text(description='Near SEGY:', value='./data/3d_nearstack.sgy')\n",
    "far_text = widgets.Text(description='Far SEGY:', value='./data/3d_farstack.sgy')\n",
    "horizon_text = widgets.Text(description='Horizon .txt:', value='./data/Top_Heimdal_subset.txt')\n",
    "\n",
    "inline_range_title = widgets.HTML(\n",
    "    value=\"<b>In-line range:<b>\",\n",
    ")\n",
    "\n",
    "xline_range_title = widgets.HTML(\n",
    "    value=\"<b>X-line range:<b>\",\n",
    ")\n",
    "\n",
    "\n",
    "inline_start = widgets.IntText(\n",
    "    value=1300,\n",
    "    description='Start:',\n",
    "    width=0.05\n",
    ")\n",
    "\n",
    "inline_stop = widgets.IntText(\n",
    "    value=1502,\n",
    "    description='Stop:',\n",
    ")\n",
    "\n",
    "inline_step = widgets.IntText(\n",
    "    value=2,\n",
    "    description='Step:',\n",
    ")\n",
    "\n",
    "xline_start = widgets.IntText(\n",
    "    value=1500,\n",
    "    description='Start:',\n",
    ")\n",
    "\n",
    "xline_stop = widgets.IntText(\n",
    "    value=2002,\n",
    "    description='Stop:',\n",
    ")\n",
    "\n",
    "xline_step = widgets.IntText(\n",
    "    value=2,\n",
    "    description='Step:',\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description='Load Data',)\n",
    "\n",
    "# Data processing:\n",
    "flattening_title = widgets.HTML(\n",
    "    value=\"<b>Horizon Flattening:<b>\",\n",
    ")\n",
    "\n",
    "norm_title = widgets.HTML(\n",
    "    value=\"<b>Normalisation:<b>\",\n",
    ")\n",
    "\n",
    "flat_option = widgets.Dropdown(\n",
    "    options=[True, False],\n",
    "    value=True,\n",
    "    description='True/False:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "above_add = widgets.IntText(\n",
    "    value=12,\n",
    "    description='Above Add:',)\n",
    "    \n",
    "below_add = widgets.IntText(\n",
    "    value=52,\n",
    "    description='Below Add:',)\n",
    "\n",
    "norm_option = widgets.Dropdown(\n",
    "    options=[True, False],\n",
    "    value=True,\n",
    "    description='True/False:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='Process Input',)\n",
    "\n",
    "\n",
    "# Model selection:\n",
    "umap_title = widgets.HTML(\n",
    "    value=\"<b>UMAP:<b>\",\n",
    ")\n",
    "\n",
    "umap_neighbours = widgets.IntText(\n",
    "    value=50,\n",
    "    description='Neighbours:',)\n",
    "\n",
    "umap_dist = widgets.BoundedFloatText(\n",
    "    value=1e-2,\n",
    "    min = 0.0001,\n",
    "    max = 1,\n",
    "    step=0.0001,\n",
    "    description='Min Dist:',)\n",
    "\n",
    "umap_button = widgets.Button(\n",
    "    description='Run Umap',)\n",
    "\n",
    "\n",
    "\n",
    "vae_title = widgets.HTML(\n",
    "    value=\"<b>VAE parameters:<b>\",\n",
    ")\n",
    "\n",
    "epoch_num = widgets.IntText(\n",
    "    value=3,\n",
    "    min=3,\n",
    "    description='Epochs:',)\n",
    "\n",
    "latent_dim = widgets.IntText(\n",
    "    value=8,\n",
    "    description='Latent size:',)\n",
    "\n",
    "learn_rate = widgets.BoundedFloatText(\n",
    "    value=1e-2,\n",
    "    min = 0.0001,\n",
    "    max = 1,\n",
    "    step=0.005,\n",
    "    description='Learn Rate:',)\n",
    "\n",
    "vae_button = widgets.Button(\n",
    "    description='Run Vae',)\n",
    "\n",
    "vumap_title = widgets.HTML(\n",
    "    value=\"<b>VAE->UMAP 2-D parameters:<b>\",\n",
    ")\n",
    "\n",
    "vumap_neighbours = widgets.IntText(\n",
    "    value=50,\n",
    "    description='Neighbours:',)\n",
    "\n",
    "vumap_dist = widgets.BoundedFloatText(\n",
    "    value=1e-2,\n",
    "    min = 0.001,\n",
    "    max = 1,\n",
    "    step=0.0001,\n",
    "    description='Min Dist:',)\n",
    "\n",
    "\n",
    "# Plotting:\n",
    "plot_title = widgets.HTML(\n",
    "    value=\"<b>Plot parameters:<b>\",\n",
    ")\n",
    "\n",
    "\n",
    "# drop down widget\n",
    "attr_dropdown = widgets.Dropdown(\n",
    "    options=['horizon', 'FF'],\n",
    "    description='Plot Attribute:',\n",
    ")\n",
    "\n",
    "# Declare output widget\n",
    "PLOToutput = widgets.Output(layout={'border': '4px solid black'})\n",
    "\n",
    "GENERALoutput = widgets.Output(layout={'border': '4px solid red'})\n",
    "\n",
    "clear_button = widgets.Button(description='Clear Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action/Logic Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     4,
     25,
     109
    ]
   },
   "outputs": [],
   "source": [
    "# LOGIC\n",
    "\n",
    "# Data load:\n",
    "@load_button.on_click\n",
    "def load_on_click(b):\n",
    "    ### Client loader\n",
    "    with GENERALoutput:\n",
    "        clear_output()\n",
    "        print('Loading SEGY: \\n')\n",
    "        \n",
    "        dataholder = DataHolder(\"Data\", [inline_start.value, inline_stop.value, inline_step.value], \n",
    "                                        [xline_start.value, xline_stop.value, xline_step.value])\n",
    "        dataholder.add_near(near_text.value);\n",
    "        dataholder.add_far(far_text.value);\n",
    "        dataholder.add_horizon(horizon_text.value)\n",
    "        dataholder.add_well('well_1', 36, 276//2)\n",
    "\n",
    "        # save to binary file\n",
    "        with open(\"./pickled/data.pickle\", \"wb\") as file_:\n",
    "            pickle.dump(dataholder, file_, -1)\n",
    "        file_.close()\n",
    "        print('\\nData Loaded successfully!\\n')\n",
    "\n",
    "# Data process:\n",
    "@process_button.on_click\n",
    "def process_on_click(b):\n",
    "    # load data\n",
    "    file_pi2 = open('./pickled/data.pickle', 'rb')\n",
    "    dataholder = pickle.load(file_pi2)\n",
    "    file_pi2.close()\n",
    "    \n",
    "    #processing\n",
    "    with GENERALoutput:\n",
    "        clear_output()\n",
    "        processor = Processor(dataholder)\n",
    "        input1 = processor([flat_option.value, above_add.value, below_add.value], norm_option.value)\n",
    "    \n",
    "        # save to binary file\n",
    "        with open(\"./pickled/input.pickle\", \"wb\") as file_:\n",
    "            pickle.dump(input1, file_, -1)\n",
    "        file_.close()\n",
    "        print('Data Input processed successfully')\n",
    "\n",
    "# Run Umap\n",
    "@umap_button.on_click\n",
    "def run_on_click(b):\n",
    "    # load input\n",
    "    file_pi2 = open('./pickled/input.pickle', 'rb')\n",
    "    input1 = pickle.load(file_pi2)\n",
    "    file_pi2.close()\n",
    "    \n",
    "    with PLOToutput:\n",
    "        clear_output()\n",
    "    \n",
    "    # UMAP RUN\n",
    "    with GENERALoutput:\n",
    "        clear_output()\n",
    "        UMAP_a = UMAP(input1)\n",
    "        UMAP_a.reduce(umap_neighbours.value)\n",
    "    \n",
    "    # First plot\n",
    "    makeplot(UMAP_a, attr_dropdown.value)\n",
    "    \n",
    "    # save to binary file\n",
    "    with open(\"./pickled/model.pickle\", \"wb\") as file_:\n",
    "        pickle.dump(UMAP_a, file_, -1)\n",
    "    file_.close()\n",
    "    \n",
    "# Run VAE\n",
    "@vae_button.on_click\n",
    "def run_on_click(b):    \n",
    "    # load input\n",
    "    file_pi2 = open('./pickled/input.pickle', 'rb')\n",
    "    input1 = pickle.load(file_pi2)\n",
    "    file_pi2.close()\n",
    "    \n",
    "    with PLOToutput:\n",
    "        clear_output()\n",
    "    \n",
    "    with GENERALoutput:\n",
    "        %matplotlib inline\n",
    "        # VAE RUN\n",
    "        Vae_a = VAE_model(input1)\n",
    "        Vae_a.reduce(epochs=epoch_num.value, hidden_size=latent_dim.value, lr=learn_rate.value,\n",
    "                     umap_neighbours=vumap_neighbours.value, umap_dist=vumap_dist.value)\n",
    "\n",
    "         # save to binary file\n",
    "        with open(\"./pickled/model.pickle\", \"wb\") as file_:\n",
    "            pickle.dump(Vae_a, file_, -1)\n",
    "        file_.close()\n",
    "        \n",
    "    \n",
    "    makeplot(Vae_a, attr_dropdown.value)\n",
    "        \n",
    "\n",
    "    \n",
    "@attr_dropdown.observe\n",
    "def on_change(change):\n",
    "    # load embedding\n",
    "    file2 = open('./pickled/model.pickle', 'rb')\n",
    "    model = pickle.load(file2)\n",
    "    file2.close()\n",
    "\n",
    "    # if dropdown change\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "#         print (change['new'])\n",
    "        makeplot(model, change['new'])\n",
    "\n",
    "    \n",
    "def makeplot(model, attr):\n",
    "    %matplotlib notebook\n",
    "    with PLOToutput:\n",
    "        clear_output()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.set(xlabel='Latent Variable 1', ylabel='Latent Variable 2',\n",
    "               title='Model used: {}, Trace Attribute: {}'.format(model.name, attr),\n",
    "               aspect='equal')\n",
    "        s = ax.scatter(model.embedding[:, 0], model.embedding[:, 1], s=0.5, c=model.attributes[attr])\n",
    "        c = plt.colorbar(s, shrink=0.7, orientation='vertical')\n",
    "        c.set_label(label=attr, rotation=90, labelpad=10)\n",
    "\n",
    "\n",
    "@clear_button.on_click\n",
    "def clear_it(arg):\n",
    "    with GENERALoutput:\n",
    "        clear_output()\n",
    "    with PLOToutput:\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI/Interface Layout Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b11159ef6e4605abcf9112aca36818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(HBox(children=(VBox(children=(HTML(value='<b>File Pathnames:<b>'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GUI LAYOUT\n",
    "%matplotlib notebook\n",
    "\n",
    "# Load tab\n",
    "tab1 = VBox(children=[HBox(children=[VBox(children=[data_files_title, near_text, far_text, horizon_text]), \n",
    "                                     VBox(children=[inline_range_title, inline_start, inline_stop, inline_step]),\n",
    "                                     VBox(children=[xline_range_title, xline_start, xline_stop, xline_step]),\n",
    "                                     ]),\n",
    "                      load_button  \n",
    "                     ])\n",
    "                     \n",
    "# processing tab\n",
    "tab2 = VBox(children=[HBox(children=[VBox(children=[flattening_title, flat_option, above_add, below_add,]),\n",
    "                                     VBox(children=[norm_title, norm_option])\n",
    "                                    ]),     \n",
    "                      process_button\n",
    "                     ])\n",
    "\n",
    "# model tab\n",
    "accordion = widgets.Accordion(children=[VBox(children=[umap_neighbours, umap_dist, umap_button]), \n",
    "                                        HBox(children=[VBox(children=[vae_title, epoch_num, learn_rate, latent_dim, vae_button]),\n",
    "                                                       VBox(children=[vumap_title, vumap_neighbours, vumap_dist])]),\n",
    "                                        VBox(children=[])])\n",
    "accordion.set_title(0, 'UMAP')\n",
    "accordion.set_title(1, 'VAE')\n",
    "accordion.set_title(2, 'β-VAE')\n",
    "\n",
    "\n",
    "tab3 = VBox(children=[HBox(children=[VBox(children=[]),\n",
    "                                     VBox(children=[]),\n",
    "                                    ]),\n",
    "                      accordion\n",
    "                     ])\n",
    "\n",
    "# visualisation\n",
    "tab4 = VBox(children=[HBox(children=[VBox(children=[plot_title, attr_dropdown]),\n",
    "                                     VBox(children=[]),\n",
    "                                    ])\n",
    "                     ])\n",
    "\n",
    "\n",
    "tab = widgets.Tab(children=[tab1, tab2, tab3, tab4])\n",
    "tab.set_title(0, '1. Data Loading')\n",
    "tab.set_title(1, '2. Data Processing')\n",
    "tab.set_title(2, '3. Model Selection')\n",
    "tab.set_title(3, '4. Visualisation')\n",
    "\n",
    "VBox(children=[tab, clear_button, PLOToutput, GENERALoutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
