{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# Dim reduction tools\n",
    "import umap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# \n",
    "from utils import *\n",
    "import segypy \n",
    "import pickle\n",
    "\n",
    "#widget imports\n",
    "from ipywidgets import interact, VBox, HBox\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy paste \"utils.py\" because it wont load/import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     12,
     21,
     34,
     41,
     58,
     70,
     79,
     80,
     105,
     115,
     125,
     137,
     142,
     155,
     183,
     200
    ]
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "def load_seismic(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    seis, header, trace_headers = segypy.readSegy(filename)\n",
    "    amplitude = seis.reshape(header['ns'], inl.size, crl.size)\n",
    "    lagtime = trace_headers['LagTimeA'][0]*-1\n",
    "    twt = np.arange(lagtime, header['dt']/1e3*header['ns']+lagtime, header['dt']/1e3)\n",
    "    return amplitude, twt\n",
    "\n",
    "def load_horizon(filename, inlines=[1300, 1502, 2], xlines=[1500, 2002, 2]):\n",
    "    inl = np.arange(*inlines)\n",
    "    crl = np.arange(*xlines)\n",
    "    hrz = np.recfromtxt(filename, names=['il','xl','z'])\n",
    "    horizon = np.zeros((len(inl), len(crl)))\n",
    "    for i, idx in enumerate(inl):\n",
    "        for j, xdx in enumerate(crl):\n",
    "            time = hrz['z'][np.where((hrz['il']== idx) & (hrz['xl'] == xdx))]\n",
    "            if len(time) == 1:\n",
    "                horizon[i, j] = time \n",
    "\n",
    "    return horizon\n",
    "\n",
    "def colorbar(mappable):\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    return fig.colorbar(mappable, cax=cax)\n",
    "\n",
    "def interpolate_horizon(horizon):\n",
    "    points = []\n",
    "    wanted = []\n",
    "    for i in range(horizon.shape[0]):\n",
    "        for j in range(horizon.shape[1]):\n",
    "            if horizon[i, j] != 0.:\n",
    "                points.append([i, j, horizon[i, j]])\n",
    "            else:\n",
    "                wanted.append([i, j])\n",
    "    \n",
    "    points = np.array(points)\n",
    "    zs2 = scipy.interpolate.griddata(points[:, 0:2], points[:, 2], wanted, method=\"cubic\")\n",
    "    for p, val in zip(wanted, zs2):\n",
    "        horizon[p[0], p[1]] = val\n",
    "    \n",
    "    return horizon\n",
    "\n",
    "def plot_section_horizon_and_well(ax, amplitude, horizon, twt, inline=38, well_pos=276//2):\n",
    "    hrz_idx = [np.abs(twt-val).argmin() for val in horizon[inline, :]]\n",
    "    \n",
    "    h_bin = np.zeros((amplitude.shape[0], amplitude.shape[2]))\n",
    "    for i, val in enumerate(hrz_idx):\n",
    "        h_bin[val, i] = 1\n",
    "\n",
    "    clip = abs(np.percentile(amplitude, 0.8))\n",
    "    ax.imshow(amplitude[:, inline], cmap=\"Greys\", vmin=-clip, vmax=clip)\n",
    "    ax.plot(range(len(hrz_idx)), hrz_idx, linewidth=5, color=\"black\")\n",
    "    ax.axvline(well_pos, color=\"red\", linewidth=5)\n",
    "\n",
    "def flatten_on_horizon(amplitude, horizon, twt, top_add=12, below_add=52):\n",
    "    traces = np.zeros((horizon.shape[0], horizon.shape[1], top_add+below_add))\n",
    "    for i in range(horizon.shape[0]):\n",
    "        hrz_idx = [np.abs(twt-val).argmin() for val in horizon[i, :]]\n",
    "        for j in range(horizon.shape[1]):\n",
    "            traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "\n",
    "    return traces\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv1d(2, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv1d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "\n",
    "        # Latent space\n",
    "        self.fc21 = nn.Linear(128, hidden_size)\n",
    "        self.fc22 = nn.Linear(128, hidden_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(hidden_size, 128)\n",
    "        self.fc4 = nn.Linear(128, 256)\n",
    "        self.deconv1 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose1d(32, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv1d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        h1 = self.relu(self.fc1(out))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            if mu.is_cuda:\n",
    "                eps = eps.cuda()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "\n",
    "        out = self.relu(self.fc4(h3))\n",
    "\n",
    "        out = out.view(out.size(0), 32, 8)\n",
    "        out = self.relu(self.deconv1(out))\n",
    "        out = self.relu(self.deconv2(out))\n",
    "        out = self.relu(self.deconv3(out))\n",
    "        out = self.conv5(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar, z\n",
    "    \n",
    "def loss_function(recon_x, x, mu, logvar, window_size=64):\n",
    "    criterion_mse = nn.MSELoss(size_average=False)\n",
    "    MSE = criterion_mse(recon_x.view(-1, 2, window_size), x.view(-1, 2, window_size))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD    \n",
    "    \n",
    "# Function to perform one epoch of training\n",
    "def train(epoch, model, optimizer, train_loader, cuda=False, log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "#         print(data.shape)\n",
    "\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, _ = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() * data.size(0) / len(train_loader.dataset)))\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "# Function to perform evaluation of data on the model, used for testing\n",
    "def test(epoch, model, test_loader, cuda=False, log_interval=10):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, _ = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item() * data.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "# Function to forward_propagate a set of tensors and receive back latent variables and reconstructions\n",
    "def forward_all(model, all_loader, cuda=False):\n",
    "    model.eval()\n",
    "    reconstructions, latents = [], []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for i, (data, _) in enumerate(all_loader):\n",
    "            if cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data)\n",
    "            recon_batch, mu, logvar, z = model(data)\n",
    "            reconstructions.append(recon_batch.cpu())\n",
    "            latents.append(z.cpu())\n",
    "    return torch.cat(reconstructions, 0), torch.cat(latents, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# testing VAE architecture\n",
    "\n",
    "# x = torch.randn((100, 2, 64))\n",
    "# print(\"init size:\", x.size())\n",
    "\n",
    "# model = VAE(hidden_size=8)\n",
    "# y = model(x)\n",
    "# # print(y)\n",
    "\n",
    "# print(\"output size\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 'BACKEND':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 DataHolder + Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     1,
     18,
     22,
     26,
     29,
     33,
     34,
     40,
     54,
     65,
     68,
     71,
     74,
     85
    ]
   },
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    def __init__(self, field_name, inlines, xlines):\n",
    "        \n",
    "        # User input attributes\n",
    "        self.field_name = field_name\n",
    "        self.inlines = inlines\n",
    "        self.xlines = xlines\n",
    "\n",
    "        # KEY data for processing\n",
    "        self.near = None\n",
    "        self.far = None\n",
    "        self.twt = None\n",
    "        self.horizon = None\n",
    "        \n",
    "        # Dictionaries for multiple possible entries\n",
    "        self.horizons = {}\n",
    "        self.wells = {}\n",
    "        \n",
    "    def add_near(self, fname):\n",
    "        self.near, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        self.twt = twt\n",
    "\n",
    "    def add_far(self, fname):\n",
    "        self.far, twt = load_seismic(fname, inlines=self.inlines, xlines=self.xlines)\n",
    "        assert (self.twt == twt).all, \"This twt does not match the twt from the previous segy\"\n",
    "        \n",
    "    def add_horizon(self, fname):\n",
    "        self.horizon = interpolate_horizon(load_horizon(fname, inlines=self.inlines, xlines=self.xlines))\n",
    "        \n",
    "    def add_well(self, well_id, well_i, well_x):\n",
    "        self.wells[well_id] = [well_i, well_x]\n",
    "           \n",
    "            \n",
    "class Processor:\n",
    "    def __init__(self, Data):\n",
    "        self.raw = [Data.near, Data.far]\n",
    "        self.twt = Data.twt\n",
    "        self.out = None\n",
    "        self.horizon = Data.horizon\n",
    "    \n",
    "    def flatten(self, data, top_add=12, below_add=52):\n",
    "        out = []\n",
    "        print('before', data[0].shape)\n",
    "        \n",
    "        for amplitude in data:\n",
    "            traces = np.zeros((self.horizon.shape[0], self.horizon.shape[1], top_add+below_add))\n",
    "            for i in range(self.horizon.shape[0]):\n",
    "                hrz_idx = [np.abs(self.twt-val).argmin() for val in self.horizon[i, :]] \n",
    "                for j in range(self.horizon.shape[1]):\n",
    "                    traces[i, j, :] = amplitude[hrz_idx[j]-top_add:hrz_idx[j]+below_add, i, j]\n",
    "            out.append(traces)\n",
    "        print('after', out[0].shape)\n",
    "        return out\n",
    "    \n",
    "    def normalise(self, data):\n",
    "        well_i=38\n",
    "        well_x=138\n",
    "        out = []\n",
    "        for i in data:\n",
    "            well_variance = np.mean(np.std(i[well_i - 2:well_i + 1, well_x - 2:well_x + 1], 2))\n",
    "            i /= well_variance\n",
    "            out.append(i)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def to_2d(self, data):\n",
    "        return [i.reshape(-1, data[0].shape[-1]) for i in data]\n",
    "        \n",
    "    def average_neighbours(self, neighbours=10):\n",
    "        return 'not implemented yet'\n",
    "          \n",
    "    def stack_traces(self, data):\n",
    "        return np.concatenate([i for i in data], 1)\n",
    "    \n",
    "    def run_AVO(self):\n",
    "#         print(self.out[0].shape, self.out[1].shape)\n",
    "        x_avo = self.out[0]\n",
    "        y_avo = self.out[1] - self.out[0]\n",
    "\n",
    "        lin_reg = LinearRegression(fit_intercept=False, normalize=False, copy_X=True, n_jobs=1)\n",
    "        lin_reg.fit(x_avo.reshape(-1, 1), y_avo.reshape(-1, 1))\n",
    "\n",
    "        print(\"Linear Regression coefficient: %1.2f\" % lin_reg.coef_[0, 0])\n",
    "        return y_avo - lin_reg.coef_ * x_avo\n",
    "    \n",
    "    def __call__(self, flatten=False, normalise=False, label='FF'):\n",
    "        self.out = copy.copy(self.raw)\n",
    "        \n",
    "        if flatten[0]:\n",
    "            self.out = self.flatten(self.out, flatten[1], flatten[2])\n",
    "        else: # perform the same axis manipulation as flattening method\n",
    "            self.out[0] = self.out[0].reshape([self.out[0].shape[1], self.out[0].shape[2], self.out[0].shape[0]])\n",
    "            self.out[1] = self.out[1].reshape([self.out[1].shape[1], self.out[1].shape[2], self.out[1].shape[0]])\n",
    "     \n",
    "        if normalise:\n",
    "            self.out = self.normalise(self.out)\n",
    "        \n",
    "        # arrays from 3d to 2d\n",
    "        self.out = self.to_2d(self.out)\n",
    "        \n",
    "        # Find fluid factor\n",
    "        self.FF = self.run_AVO()\n",
    "            \n",
    "        # Stack the traces for output\n",
    "        self.out = self.stack_traces(self.out)\n",
    "        \n",
    "        return [self.out, self.FF] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ModelAgent -> UMAP + VAE + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     16,
     31
    ]
   },
   "outputs": [],
   "source": [
    "class ModelAgent:\n",
    "    def __init__(self, data):\n",
    "        self.input = data[0]\n",
    "        self.FF = data[1]\n",
    "        self.embedding = None\n",
    "        print(\"ModelAgent initialised\")\n",
    "        \n",
    "    def plot_2d(self, data, label):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "        sc = ax.scatter(data[:, 0], data[:, 1], s=2.0, c=np.min(label, 1))\n",
    "        return 1\n",
    "        \n",
    "    def plot_3d(self, data, feature):\n",
    "        return 'Not implemented'\n",
    "        \n",
    "        \n",
    "class UMAP(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "\n",
    "    \n",
    "    def reduce(self, n_neighbors = 50, min_dist=0.001):\n",
    "        embedding = umap.UMAP(n_neighbors=n_neighbors,\n",
    "                      min_dist=min_dist,\n",
    "                      metric='correlation', \n",
    "                               verbose=True,\n",
    "                            random_state=42).fit_transform(self.input)\n",
    "        self.embedding = embedding        \n",
    "        return embedding\n",
    "        \n",
    "    \n",
    "class VAE_model(ModelAgent):\n",
    "    def __init__(self, data):\n",
    "        super().__init__(data)\n",
    "    \n",
    "    def create_dataloader(self, batch_size=32):\n",
    "        # split the concatenated input back into two arrays\n",
    "        X = torch.from_numpy(np.stack(np.split(self.input, 2, axis=1), 1)).float()\n",
    "        \n",
    "        # Create a stacked representation and a zero tensor so we can use the standard Pytorch TensorDataset\n",
    "        y = torch.from_numpy(np.zeros((X.shape[0], 1))).float()\n",
    "        \n",
    "        split = ShuffleSplit(n_splits=1, test_size=0.5)\n",
    "        for train_index, test_index in split.split(X):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "        train_dset = TensorDataset(X_train, y_train)\n",
    "        test_dset = TensorDataset(X_test, y_test)\n",
    "        all_dset = TensorDataset(X, y)\n",
    "\n",
    "        kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        self.all_loader = torch.utils.data.DataLoader(all_dset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        \n",
    "    def train_vae(self, cuda=False, epochs=5, hidden_size=8):\n",
    "        set_seed(42)  # Set the random seed\n",
    "        self.model = VAE(hidden_size)  # Inititalize the model\n",
    "\n",
    "        # use cuda if chosen\n",
    "        if cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "        # Create a gradient descent optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "\n",
    "        # Store and plot losses\n",
    "        self.losses = []\n",
    "\n",
    "        # Start training loop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tl = train(epoch, self.model, optimizer, self.train_loader, cuda=False)  # Train model on train dataset\n",
    "            testl = test(epoch, self.model, self.test_loader, cuda=False)  # Validate model on test dataset\n",
    "            self.losses.append([tl, testl])\n",
    "        \n",
    "    def run_vae(self):\n",
    "        _, self.zs = forward_all(self.model, self.all_loader, cuda=False)\n",
    "        \n",
    "    def vae_umap(self):\n",
    "        transformer = umap.UMAP(n_neighbors=5,\n",
    "                                min_dist=0.001,\n",
    "                                metric='correlation', verbose=True).fit(self.zs.numpy())\n",
    "        embedding = transformer.transform(self.zs.numpy())\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    def reduce(self, epochs, hidden_size):\n",
    "        self.create_dataloader()\n",
    "        self.train_vae(epochs=epochs, hidden_size=hidden_size)\n",
    "        self.run_vae()\n",
    "        self.embedding = self.vae_umap()\n",
    "        #self.plot_2d(self.embedding, self.attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 PlottingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# not yet instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BASIS API  example usage (for manual testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Client loader\n",
    "# dataholder = DataHolder(\"Glitne\", [1300, 1502, 2], [1500, 2002, 2])\n",
    "# dataholder.add_near('./data/3d_nearstack.sgy');\n",
    "# dataholder.add_far('./data/3d_farstack.sgy');\n",
    "# dataholder.add_horizon('./data/Top_Heimdal_subset.txt')\n",
    "# dataholder.add_well('well_1', 36, 276//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Processor\n",
    "# processor1 = Processor(dataholder)\n",
    "\n",
    "# Input1 = processor1([False, 12,52], normalise=True)\n",
    "# Input2 = processor1([True, 12,52], normalise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### UMAP RUN\n",
    "# UMAP_a = UMAP(Input1)\n",
    "# UMAP_a1 = UMAP_a.reduce(n_neighbors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VAE RUN\n",
    "# VAE_a = VAE_model(Input1)\n",
    "\n",
    "# run with different params\n",
    "# VAE_a.create_dataloader()\n",
    "# VAE_a.train_vae()\n",
    "# VAE_a.run_vae()\n",
    "# VAE_a.vae_umap()\n",
    "# VAE_a.reduce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. WIDGET INTERFACE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Widget Definitions\n",
    "\n",
    "# Data Loading:\n",
    "data_files_title = widgets.HTML(\n",
    "    value=\"<b>File Pathnames:<b>\",\n",
    ")\n",
    "\n",
    "near_text = widgets.Text(description='Near SEGY:', value='./data/3d_nearstack.sgy')\n",
    "far_text = widgets.Text(description='Far SEGY:', value='./data/3d_farstack.sgy')\n",
    "horizon_text = widgets.Text(description='Horizon .txt:', value='./data/Top_Heimdal_subset.txt')\n",
    "\n",
    "inline_range_title = widgets.HTML(\n",
    "    value=\"<b>In-line range:<b>\",\n",
    ")\n",
    "\n",
    "xline_range_title = widgets.HTML(\n",
    "    value=\"<b>X-line range:<b>\",\n",
    ")\n",
    "\n",
    "\n",
    "inline_start = widgets.IntText(\n",
    "    value=1300,\n",
    "    description='Start:',\n",
    "    width=0.05\n",
    ")\n",
    "\n",
    "inline_stop = widgets.IntText(\n",
    "    value=1502,\n",
    "    description='Stop:',\n",
    ")\n",
    "\n",
    "inline_step = widgets.IntText(\n",
    "    value=2,\n",
    "    description='Step:',\n",
    ")\n",
    "\n",
    "xline_start = widgets.IntText(\n",
    "    value=1500,\n",
    "    description='Start:',\n",
    ")\n",
    "\n",
    "xline_stop = widgets.IntText(\n",
    "    value=2002,\n",
    "    description='Stop:',\n",
    ")\n",
    "\n",
    "xline_step = widgets.IntText(\n",
    "    value=2,\n",
    "    description='Step:',\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description='Load Data',)\n",
    "\n",
    "# Data processing:\n",
    "flattening_title = widgets.HTML(\n",
    "    value=\"<b>Horizon Flattening:<b>\",\n",
    ")\n",
    "\n",
    "norm_title = widgets.HTML(\n",
    "    value=\"<b>Normalisation:<b>\",\n",
    ")\n",
    "\n",
    "flat_option = widgets.Dropdown(\n",
    "    options=[True, False],\n",
    "    value=True,\n",
    "    description='True/False:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "above_add = widgets.IntText(\n",
    "    value=12,\n",
    "    description='Above Add:',)\n",
    "    \n",
    "below_add = widgets.IntText(\n",
    "    value=52,\n",
    "    description='Below Add:',)\n",
    "\n",
    "norm_option = widgets.Dropdown(\n",
    "    options=[True, False],\n",
    "    value=True,\n",
    "    description='True/False:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='Process Input',)\n",
    "\n",
    "\n",
    "# Model selection:\n",
    "umap_title = widgets.HTML(\n",
    "    value=\"<b>UMAP:<b>\",\n",
    ")\n",
    "\n",
    "umap_neighbours = widgets.IntText(\n",
    "    value=50,\n",
    "    description='Neighbours:',)\n",
    "\n",
    "umap_button = widgets.Button(\n",
    "    description='Run Umap',)\n",
    "\n",
    "\n",
    "\n",
    "vae_title = widgets.HTML(\n",
    "    value=\"<b>VAE:<b>\",\n",
    ")\n",
    "\n",
    "epoch_num = widgets.IntText(\n",
    "    value=3,\n",
    "    description='Epochs:',)\n",
    "\n",
    "latent_dim = widgets.IntText(\n",
    "    value=8,\n",
    "    description='Latent size:',)\n",
    "\n",
    "vae_button = widgets.Button(\n",
    "    description='Run Vae',)\n",
    "\n",
    "\n",
    "# Plotting:\n",
    "plot_title = widgets.HTML(\n",
    "    value=\"<b>Plot parameters:<b>\",\n",
    ")\n",
    "plot_button = widgets.Button(\n",
    "    description='Plot',)\n",
    "\n",
    "\n",
    "attribute_toggle = widgets.ToggleButtons(\n",
    "    options=['AVO ff', 'Horizon \"Depth\"'],\n",
    "    description='Attribute:',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltips=['Description of slow', 'Description of regular'],\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# scrap:\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "clear_button = widgets.Button(description='Clear Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action/Logic Definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     39,
     72
    ]
   },
   "outputs": [],
   "source": [
    "# LOGIC\n",
    "\n",
    "# Data load:\n",
    "@load_button.on_click\n",
    "def load_on_click(b):\n",
    "    ### Client loader\n",
    "    dataholder = DataHolder(\"Data\", [inline_start.value, inline_stop.value, inline_step.value], \n",
    "                                    [xline_start.value, xline_stop.value, xline_step.value])\n",
    "    dataholder.add_near(near_text.value);\n",
    "    dataholder.add_far(far_text.value);\n",
    "    dataholder.add_horizon(horizon_text.value)\n",
    "    dataholder.add_well('well_1', 36, 276//2)\n",
    "    \n",
    "    # save to binary file\n",
    "    with open(\"./pickled/data.pickle\", \"wb\") as file_:\n",
    "        pickle.dump(dataholder, file_, -1)\n",
    "    file_.close()\n",
    "    print('Data Loaded successfully')\n",
    "\n",
    "# Data process:\n",
    "@process_button.on_click\n",
    "def process_on_click(b):\n",
    "    # load data\n",
    "    file_pi2 = open('./pickled/data.pickle', 'rb')\n",
    "    dataholder = pickle.load(file_pi2)\n",
    "    file_pi2.close()\n",
    "    \n",
    "    #processing\n",
    "    processor = Processor(dataholder)\n",
    "    input1 = processor([flat_option.value, above_add.value, below_add.value], norm_option.value)\n",
    "    \n",
    "    # save to binary file\n",
    "    with open(\"./pickled/input.pickle\", \"wb\") as file_:\n",
    "        pickle.dump(input1, file_, -1)\n",
    "    file_.close()\n",
    "    print('Data Input processed successfully')\n",
    "\n",
    "# Run Umap\n",
    "@umap_button.on_click\n",
    "def run_on_click(b):\n",
    "    # load input\n",
    "    file_pi2 = open('./pickled/input.pickle', 'rb')\n",
    "    input1 = pickle.load(file_pi2)\n",
    "    file_pi2.close()\n",
    "    # UMAP RUN\n",
    "    UMAP_a = UMAP(input1)\n",
    "    UMAP_a.reduce(umap_neighbours.value)\n",
    "    \n",
    "    # save to binary file\n",
    "    with open(\"./pickled/model.pickle\", \"wb\") as file_:\n",
    "        pickle.dump(UMAP_a, file_, -1)\n",
    "    file_.close()\n",
    "    \n",
    "# Run VAE\n",
    "@vae_button.on_click\n",
    "def run_on_click(b):\n",
    "    # load input\n",
    "    file_pi2 = open('./pickled/input.pickle', 'rb')\n",
    "    input1 = pickle.load(file_pi2)\n",
    "    file_pi2.close()\n",
    "    \n",
    "    # VAE RUN\n",
    "    VAE_a = VAE_model(input1)\n",
    "    VAE_a.reduce(epochs=epoch_num.value, hidden_size=latent_dim.value)\n",
    "    \n",
    "     # save to binary file\n",
    "    with open(\"./pickled/model.pickle\", \"wb\") as file_:\n",
    "        pickle.dump(VAE_a, file_, -1)\n",
    "    file_.close()\n",
    "    \n",
    "#Plot\n",
    "@plot_button.on_click\n",
    "def plot_on_click(b):\n",
    "    # load data holder\n",
    "    file1 = open('./pickled/data.pickle', 'rb')\n",
    "    dataholder = pickle.load(file1)\n",
    "    file1.close()\n",
    "    \n",
    "    # load embedding\n",
    "    file2 = open('./pickled/model.pickle', 'rb')\n",
    "    model = pickle.load(file2)\n",
    "    file2.close()\n",
    "    \n",
    "    if attribute_toggle.value == 'Horizon \"Depth\"':\n",
    "        print('Horizon attribute')\n",
    "        attr = dataholder.horizon.reshape(25351)\n",
    "        \n",
    "    elif attribute_toggle.value == 'AVO ff':\n",
    "        print('AVO attribute')\n",
    "        attr = np.min(model.FF, axis=(1))\n",
    "        \n",
    "    else: print('error attr input not recognised')\n",
    "    \n",
    "    with out:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "        sc = ax.scatter(model.embedding[::, 0], model.embedding[::, 1], s=2.0, c=attr)\n",
    "        print('plotted')\n",
    "\n",
    "@clear_button.on_click\n",
    "def clear_on_click(b):\n",
    "    out.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI/Interface Layout Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4b2f014de04698956fc1c04c7b5d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(HBox(children=(VBox(children=(HTML(value='<b>File Pathnames:<b>'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GUI LAYOUT\n",
    "tab1 = VBox(children=[HBox(children=[VBox(children=[data_files_title, near_text, far_text, horizon_text]), \n",
    "                                     VBox(children=[inline_range_title, inline_start, inline_stop, inline_step]),\n",
    "                                     VBox(children=[xline_range_title, xline_start, xline_stop, xline_step]),\n",
    "                                     ]),\n",
    "                      load_button  \n",
    "                     ])\n",
    "                     \n",
    "\n",
    "tab2 = VBox(children=[HBox(children=[VBox(children=[flattening_title, flat_option, above_add, below_add,]),\n",
    "                                     VBox(children=[norm_title, norm_option])\n",
    "                                    ]),     \n",
    "                      process_button\n",
    "                     ])\n",
    "\n",
    "tab3 = VBox(children=[HBox(children=[VBox(children=[umap_title, umap_neighbours, umap_button]),\n",
    "                                     VBox(children=[vae_title, epoch_num, latent_dim, vae_button]),\n",
    "                                    ])\n",
    "                     ])\n",
    "\n",
    "tab4 = VBox(children=[HBox(children=[VBox(children=[plot_title, attribute_toggle, plot_button]),\n",
    "                                     VBox(children=[]),\n",
    "                                    ])\n",
    "                     ])\n",
    "\n",
    "\n",
    "tab = widgets.Tab(children=[tab1, tab2, tab3, tab4])\n",
    "tab.set_title(0, '1. Data Loading')\n",
    "tab.set_title(1, '2. Data Processing')\n",
    "tab.set_title(2, '3. Model Selection')\n",
    "tab.set_title(3, '4. Visualisation')\n",
    "\n",
    "VBox(children=[tab, clear_button, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
